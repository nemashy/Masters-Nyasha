{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "import scipy.signal as signal\r\n",
    "import numpy as np\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import time\r\n",
    "from torch2trt import TRTModule\r\n",
    "from torch2trt import torch2trt\r\n",
    "from models.CNN import ErnNet\r\n",
    "from utils import get_pytorch_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_start_points_x(width, slice_width, overlap_x):\r\n",
    "    x_points = [0]\r\n",
    "    stride = int(slice_width * (1 - overlap_x))\r\n",
    "    counter = 1\r\n",
    "    while True:\r\n",
    "        pt = stride * counter\r\n",
    "        if pt + slice_width >= width:\r\n",
    "            x_points.append(width - slice_width)\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            x_points.append(pt)\r\n",
    "        counter += 1\r\n",
    "    return x_points\r\n",
    "\r\n",
    "def get_total_inference_time(self, model, num_loops):\r\n",
    "    \"\"\"Returns the total inference time on all the loops\"\"\"\r\n",
    "    # Wait for all kernels in all streams on the CUDA device to complete.\r\n",
    "    torch.cuda.current_stream().synchronize()\r\n",
    "\r\n",
    "    # GPU warmup\r\n",
    "    for _ in range(10):\r\n",
    "        _ = model(self.input_data_batch)\r\n",
    "\r\n",
    "    t0 = time.time()\r\n",
    "    for _ in range(num_loops):\r\n",
    "        _ = model(self.input_data_batch)\r\n",
    "        torch.cuda.current_stream().synchronize()\r\n",
    "    t1 = time.time()\r\n",
    "\r\n",
    "    return t1 - t0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load model\r\n",
    "model_trt = TRTModule()\r\n",
    "model_trt.load_state_dict(torch.load('trt_models/model_trt_int8.pth'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "torch.cuda.current_stream().synchronize()\r\n",
    "num_imgs = 0\r\n",
    "t0 = time.time()\r\n",
    "for _ in range(1000):\r\n",
    "    fs = 714 # Sampling frequency\r\n",
    "    nperseg = 128 # Length of each segment\r\n",
    "    noverlap = 64 # Number of overlapping points between segments\r\n",
    "    nfft = 128 # Length of FFT\r\n",
    "\r\n",
    "    x = np.random.random(400000) + np.random.random(400000) * 1j\r\n",
    "\r\n",
    "    f, t, Zxx = signal.stft(x, fs=fs, window='hamming', nperseg=nperseg, noverlap=noverlap, nfft=nfft, detrend=False, return_onesided=True, boundary='zeros', padded=True, axis=- 1)\r\n",
    "    # plt.pcolormesh(t, f, np.abs(Zxx))\r\n",
    "\r\n",
    "    fft_dB =20*np.log10(np.abs(Zxx))\r\n",
    "    # fft_dB.shape\r\n",
    "\r\n",
    "    width = fft_dB.shape[1]\r\n",
    "    slice_width = 45\r\n",
    "    overlap = 0.5\r\n",
    "\r\n",
    "    start_points = get_start_points_x(width, slice_width, overlap)\r\n",
    "\r\n",
    "    segments = torch.from_numpy(np.array([fft_dB[:,start_point:start_point+slice_width] for start_point in start_points], dtype=np.float32))\r\n",
    "    segments_shape = segments.shape\r\n",
    "\r\n",
    "    reshaped_segments = torch.reshape(segments, (segments_shape[0],1, segments_shape[1], segments_shape[2])).cuda()\r\n",
    "    model_trt(reshaped_segments)\r\n",
    "    num_imgs += len(reshaped_segments)\r\n",
    "\r\n",
    "t1 = time.time()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/nyasha/anaconda3/envs/thesis/lib/python3.8/site-packages/scipy/signal/spectral.py:1812: UserWarning: Input data is complex, switching to return_onesided=False\n",
      "  warnings.warn('Input data is complex, switching to '\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "throughput = num_imgs/(t1-t0)\r\n",
    "point_throughput = throughput * 45\r\n",
    "supported_range_bins = point_throughput/fs\r\n",
    "supported_range_bins*5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1969.864781229934"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('thesis': conda)"
  },
  "interpreter": {
   "hash": "c038a02319e44538776f88f6eccb85a8daa469fb892b6db8d0a399f3ecc69bfa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}